{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing SQL Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_df = sqlCtx.read.parquet(\"data/full_df.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+-------+--------------+-------------------+--------------+--------+-----------------+---------------+-----+\n",
      "|MONTH|DAY|DAY_OF_WEEK|AIRLINE|ORIGIN_AIRPORT|SCHEDULED_DEPARTURE|SCHEDULED_TIME|DISTANCE|SCHEDULED_ARRIVAL|DEPARTURE_DELAY|DELAY|\n",
      "+-----+---+-----------+-------+--------------+-------------------+--------------+--------+-----------------+---------------+-----+\n",
      "|    3| 22|          7|     13|            95|               1405|         190.0|    1216|             1715|          -23.0|    0|\n",
      "|   11| 29|          7|      1|             5|               1521|          64.0|     192|             1625|           12.0|    1|\n",
      "|    7| 22|          3|      9|           223|               1245|          85.0|     331|             1410|           -5.0|    0|\n",
      "|   11| 17|          2|      2|            15|                815|         207.0|    1547|             1242|            1.0|    1|\n",
      "|   11| 30|          1|      2|           163|                635|         183.0|     952|              938|            4.0|    1|\n",
      "+-----+---+-----------+-------+--------------+-------------------+--------------+--------+-----------------+---------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['MONTH', 'DAY', 'DAY_OF_WEEK','AIRLINE',\n",
    "                                       'SCHEDULED_DEPARTURE', 'ORIGIN_AIRPORT', 'SCHEDULED_TIME', 'DISTANCE', \n",
    "                                       'SCHEDULED_ARRIVAL'], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = assembler.transform(flight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|DELAY|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[3.0,22.0,7.0,13....|\n",
      "|    1|[11.0,29.0,7.0,1....|\n",
      "|    0|[7.0,22.0,3.0,9.0...|\n",
      "|    1|[11.0,17.0,2.0,2....|\n",
      "|    1|[11.0,30.0,1.0,2....|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.select(['DELAY', 'features']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD = transformed.select(['DELAY','features']).rdd.map(tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = dataRDD.map(lambda row : (0 if row[0] == 0 else 1, Vectors.dense(row[1])))    \\\n",
    "            .map(lambda row : LabeledPoint(row[0], row[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [3.0,22.0,7.0,13.0,1405.0,95.0,190.0,1216.0,1715.0]),\n",
       " LabeledPoint(1.0, [11.0,29.0,7.0,1.0,1521.0,5.0,64.0,192.0,1625.0]),\n",
       " LabeledPoint(0.0, [7.0,22.0,3.0,9.0,1245.0,223.0,85.0,331.0,1410.0]),\n",
       " LabeledPoint(1.0, [11.0,17.0,2.0,2.0,815.0,15.0,207.0,1547.0,1242.0]),\n",
       " LabeledPoint(1.0, [11.0,30.0,1.0,2.0,635.0,163.0,183.0,952.0,938.0]),\n",
       " LabeledPoint(0.0, [6.0,20.0,6.0,3.0,1945.0,5.0,95.0,447.0,2120.0]),\n",
       " LabeledPoint(1.0, [2.0,23.0,1.0,10.0,2000.0,21.0,73.0,184.0,2113.0]),\n",
       " LabeledPoint(1.0, [2.0,27.0,5.0,7.0,623.0,133.0,154.0,723.0,757.0]),\n",
       " LabeledPoint(1.0, [11.0,2.0,1.0,4.0,1055.0,15.0,119.0,666.0,1254.0]),\n",
       " LabeledPoint(1.0, [4.0,10.0,5.0,9.0,2145.0,15.0,92.0,448.0,2217.0]),\n",
       " LabeledPoint(0.0, [2.0,13.0,5.0,3.0,1205.0,142.0,75.0,220.0,1320.0])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.take(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = lp.randomSplit([0.8, 0.2], 314)\n",
    "training = split[0]\n",
    "test = split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_model = LogisticRegressionWithLBFGS.train(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_LAP = test.map(lambda lp: (float(LR_model.predict(lp.features)), lp.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584769860654\n"
     ]
    }
   ],
   "source": [
    "LR_acc = 1.0 * LR_LAP.filter(lambda x:x[0] == x[1]).count()/test.count()\n",
    "print(LR_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_model = RandomForest.trainClassifier(training, numClasses = 2,\n",
    "                                       categoricalFeaturesInfo = {}, \n",
    "                                       numTrees = 5, featureSubsetStrategy = \"auto\", \n",
    "                                       impurity = 'gini', maxDepth = 4, maxBins = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_pred = RF_model.predict(test.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_LAP = test.map(lambda lp: lp.label).zip(RF_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.600604108409\n"
     ]
    }
   ],
   "source": [
    "RF_testErr = RF_LAP.filter(lambda x: x[0] == x[1]).count() / float(test.count())\n",
    "print(RF_testErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_model = DecisionTree.trainClassifier(training, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5, maxBins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_pred = DT_model.predict(test.map(lambda x: x.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DT_LAP = test.map(lambda lp: lp.label).zip(DT_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.605099524501\n"
     ]
    }
   ],
   "source": [
    "DT_LAP = test.map(lambda lp: lp.label).zip(DT_pred)\n",
    "DT_testErr = DT_LAP.filter(lambda x: x[0] == x[1]).count() / float(test.count())\n",
    "print(DT_testErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_model = SVMWithSGD.train(training, iterations = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SVM_LAP = test.map(lambda x: (float(SVM_model.predict(x.features)), x.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.501716753389\n"
     ]
    }
   ],
   "source": [
    "SVM_testErr = SVM_LAP.filter(lambda x: x[0] == x[1]).count()/float(test.count())\n",
    "print(SVM_testErr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_k = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_k = ParamGridBuilder().addGrid(lr_k.maxIter, [0, 1]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluator_k = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_lr = CrossValidator(estimator = lr_k, estimatorParamMaps = grid_k, evaluator = evaluator_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[3.0,22.0,7.0,13....|\n",
      "|  1.0|[11.0,29.0,7.0,1....|\n",
      "|  0.0|[7.0,22.0,3.0,9.0...|\n",
      "|  1.0|[11.0,17.0,2.0,2....|\n",
      "|  1.0|[11.0,30.0,1.0,2....|\n",
      "|  0.0|[6.0,20.0,6.0,3.0...|\n",
      "|  1.0|[2.0,23.0,1.0,10....|\n",
      "|  1.0|[2.0,27.0,5.0,7.0...|\n",
      "|  1.0|[11.0,2.0,1.0,4.0...|\n",
      "|  1.0|[4.0,10.0,5.0,9.0...|\n",
      "|  0.0|[2.0,13.0,5.0,3.0...|\n",
      "|  1.0|[7.0,25.0,6.0,4.0...|\n",
      "|  0.0|[4.0,22.0,3.0,2.0...|\n",
      "|  0.0|[6.0,19.0,5.0,2.0...|\n",
      "|  1.0|[3.0,26.0,4.0,9.0...|\n",
      "|  1.0|[4.0,1.0,3.0,7.0,...|\n",
      "|  0.0|[1.0,17.0,6.0,9.0...|\n",
      "|  1.0|[7.0,11.0,6.0,1.0...|\n",
      "|  0.0|[1.0,11.0,7.0,3.0...|\n",
      "|  0.0|[8.0,23.0,7.0,4.0...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flight_cv = transformed.select(['DELAY', 'features'])\n",
    "flight_cv = flight_cv.withColumnRenamed('DELAY', 'label')\n",
    "flight_cv = flight_cv.select(flight_cv.label.cast(DoubleType()).alias('label'), \n",
    "                                 'features')\n",
    "flight_cv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cv, test_cv = flight_cv.randomSplit([0.8, 0.2], 314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark-1.6.1-bin-hadoop2.6/python/pyspark/ml/classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n",
      "  warnings.warn(\"weights is deprecated. Use coefficients instead.\")\n"
     ]
    }
   ],
   "source": [
    "cvmodel_lr = cv_lr.fit(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222040497876321"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_k.evaluate(cvmodel_lr.transform(train_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorIndexer, IndexToString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol = \"label\", \n",
    "                             outputCol = \"indexedLabel\").fit(flight_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featureIndexer = VectorIndexer(inputCol=\"features\", \n",
    "                              outputCol=\"indexedFeatures\", \n",
    "                              maxCategories=4).fit(flight_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_k = RandomForestClassifier(labelCol = \"indexedLabel\", \n",
    "                              featuresCol = \"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\",\n",
    "                               outputCol=\"predictedLabel\", \n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\",\n",
    "                                                 predictionCol=\"prediction\")\n",
    "numFolds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline(stages=[labelIndexer, \n",
    "                               featureIndexer,\n",
    "                               rf_k,\n",
    "                               labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_rf = CrossValidator(estimator = pipeline_rf, \n",
    "                       estimatorParamMaps = grid_k, \n",
    "                       evaluator = evaluator_rf, \n",
    "                       numFolds = numFolds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cvmodel_rf = cv_rf.fit(train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_rf = cvmodel_rf.transform(test_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|[1.0,1.0,4.0,1.0,...|\n",
      "|           0.0|  0.0|[1.0,1.0,4.0,1.0,...|\n",
      "|           1.0|  0.0|[1.0,1.0,4.0,2.0,...|\n",
      "|           1.0|  0.0|[1.0,1.0,4.0,3.0,...|\n",
      "|           1.0|  0.0|[1.0,1.0,4.0,3.0,...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_rf.select(\"predictedLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071491674276406"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator_rf.evaluate(predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3861788226173339"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluator_rf.evaluate(cvmodel_rf.transform(train_cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
